{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGS: \n",
    "## N = 500\n",
    "### Nelder-Mead : время работы 290 секунд, Gini: train 0.8, test 0.77\n",
    "### differential-evolution : время работы порядка 15 минут, Gini: train 0.76, test 0.75 -- default params\n",
    "### BFGS : время работы 95 секунд, Gini: train 0.8, test 0.76 -- default params\n",
    "### CG : время работы 127 секунд, Gini: train 0.77, test 0.75 -- default params\n",
    "### SLSQP : время работы 42 секунд, Gini: train 0.81, test 0.776 -- default params\n",
    "### COBYLA : время работы 84 секунд, Gini: train 0.75, test 0.75 -- default params\n",
    "### TNC : время работы 590 секунд, Gini: train 0.82, test 0.80 -- default params\n",
    "\n",
    "## N = 200\n",
    "### TNC 14 hyperplanes : время работы 834 секунд, Gini: train 0.85, test 0.83 -- default params\n",
    "\n",
    "## N = 100\n",
    "### TNC 21 hyperplanes : время работы 984 секунд, Gini: train 0.885, test 0.85 -- default params\n",
    "\n",
    "## Сравнение с другими моделями:\n",
    "### RandomForestClassifier: Gini: train 1.0, test 0.80 -- default params\n",
    "### LGBMClassifier: Gini: train 0.998, test 0.86 -- default params\n",
    "### CatBoostClassifier: Gini: train 0.997, test 0.89 -- default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Устойчивост эл_сети.xlsx\",engine='openpyxl',sheet_name='Data_for_UCI_named')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2.930406</td>\n",
       "      <td>9.487627</td>\n",
       "      <td>2.376523</td>\n",
       "      <td>6.187797</td>\n",
       "      <td>3.343416</td>\n",
       "      <td>-0.658054</td>\n",
       "      <td>-1.449106</td>\n",
       "      <td>-1.236256</td>\n",
       "      <td>0.601709</td>\n",
       "      <td>0.779642</td>\n",
       "      <td>0.813512</td>\n",
       "      <td>0.608385</td>\n",
       "      <td>0.023892</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3.392299</td>\n",
       "      <td>1.274827</td>\n",
       "      <td>2.954947</td>\n",
       "      <td>6.894759</td>\n",
       "      <td>4.349512</td>\n",
       "      <td>-1.663661</td>\n",
       "      <td>-0.952437</td>\n",
       "      <td>-1.733414</td>\n",
       "      <td>0.502079</td>\n",
       "      <td>0.567242</td>\n",
       "      <td>0.285880</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2.364034</td>\n",
       "      <td>2.842030</td>\n",
       "      <td>8.776391</td>\n",
       "      <td>1.008906</td>\n",
       "      <td>4.299976</td>\n",
       "      <td>-1.380719</td>\n",
       "      <td>-0.943884</td>\n",
       "      <td>-1.975373</td>\n",
       "      <td>0.487838</td>\n",
       "      <td>0.986505</td>\n",
       "      <td>0.149286</td>\n",
       "      <td>0.145984</td>\n",
       "      <td>-0.031810</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9.631511</td>\n",
       "      <td>3.994398</td>\n",
       "      <td>2.757071</td>\n",
       "      <td>7.821347</td>\n",
       "      <td>2.514755</td>\n",
       "      <td>-0.966330</td>\n",
       "      <td>-0.649915</td>\n",
       "      <td>-0.898510</td>\n",
       "      <td>0.365246</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.889118</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6.530527</td>\n",
       "      <td>6.781790</td>\n",
       "      <td>4.349695</td>\n",
       "      <td>8.673138</td>\n",
       "      <td>3.492807</td>\n",
       "      <td>-1.390285</td>\n",
       "      <td>-1.532193</td>\n",
       "      <td>-0.570329</td>\n",
       "      <td>0.073056</td>\n",
       "      <td>0.505441</td>\n",
       "      <td>0.378761</td>\n",
       "      <td>0.942631</td>\n",
       "      <td>0.045263</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "9995  2.930406  9.487627  2.376523  6.187797  3.343416 -0.658054 -1.449106   \n",
       "9996  3.392299  1.274827  2.954947  6.894759  4.349512 -1.663661 -0.952437   \n",
       "9997  2.364034  2.842030  8.776391  1.008906  4.299976 -1.380719 -0.943884   \n",
       "9998  9.631511  3.994398  2.757071  7.821347  2.514755 -0.966330 -0.649915   \n",
       "9999  6.530527  6.781790  4.349695  8.673138  3.492807 -1.390285 -1.532193   \n",
       "\n",
       "            p4        g1        g2        g3        g4      stab     stabf  \n",
       "9995 -1.236256  0.601709  0.779642  0.813512  0.608385  0.023892  unstable  \n",
       "9996 -1.733414  0.502079  0.567242  0.285880  0.366120 -0.025803    stable  \n",
       "9997 -1.975373  0.487838  0.986505  0.149286  0.145984 -0.031810    stable  \n",
       "9998 -0.898510  0.365246  0.587558  0.889118  0.818391  0.037789  unstable  \n",
       "9999 -0.570329  0.073056  0.505441  0.378761  0.942631  0.045263  unstable  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'unstable': 0, 'stable': 1}\n",
    "df.stabf = df.stabf.map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>-0.005957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4      stab  stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347      0  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957      1  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471      0  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871      0  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['stab', 'stabf'], axis=1).values\n",
    "y = df.stabf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]):\n",
    "    X[:,i]=(X[:,i]-X[:,i].min())/(X[:,i].max()-X[:,i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36557142857142855"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3536666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini = 0.9444212289143077\n",
      "Test Gini = 0.7496119874844391\n",
      "Train F1 = 0.8987566607460036\n",
      "Test F1 = 0.7863894139886579\n",
      "Train Accuracy = 0.9267142857142857\n",
      "Test Accuracy = 0.8493333333333334\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=8)\n",
    "tree.fit(X_train, y_train)\n",
    "train_proba = tree.predict_proba(X_train)\n",
    "test_proba = tree.predict_proba(X_test)\n",
    "train_preds = tree.predict(X_train)\n",
    "test_preds = tree.predict(X_test)\n",
    "print('Train Gini = {}'.format(2 * roc_auc_score(y_train, train_proba[:, 1]) - 1))\n",
    "print('Test Gini = {}'.format(2 * roc_auc_score(y_test, test_proba[:, 1]) - 1))\n",
    "print('Train F1 = {}'.format(f1_score(y_train, train_preds)))\n",
    "print('Test F1 = {}'.format(f1_score(y_test, test_preds)))\n",
    "print('Train Accuracy = {}'.format(accuracy_score(y_train, train_preds)))\n",
    "print('Test Accuracy = {}'.format(accuracy_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini = 0.9444212289143077\n",
      "Test Gini = 0.7496119874844391\n",
      "Train F1 = 0.7404549147034932\n",
      "Test F1 = 0.7221954742416947\n",
      "Train Accuracy = 0.8174285714285714\n",
      "Test Accuracy = 0.8076666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=100)\n",
    "logreg.fit(X_train, y_train)\n",
    "train_preds = logreg.predict_proba(X_train)\n",
    "test_preds = logreg.predict_proba(X_test)\n",
    "train_preds = logreg.predict(X_train)\n",
    "test_preds = logreg.predict(X_test)\n",
    "print('Train Gini = {}'.format(2 * roc_auc_score(y_train, train_proba[:, 1]) - 1))\n",
    "print('Test Gini = {}'.format(2 * roc_auc_score(y_test, test_proba[:, 1]) - 1))\n",
    "print('Train F1 = {}'.format(f1_score(y_train, train_preds)))\n",
    "print('Test F1 = {}'.format(f1_score(y_test, test_preds)))\n",
    "print('Train Accuracy = {}'.format(accuracy_score(y_train, train_preds)))\n",
    "print('Test Accuracy = {}'.format(accuracy_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini = 0.9444212289143077\n",
      "Test Gini = 0.7496119874844391\n",
      "Train F1 = 1.0\n",
      "Test F1 = 0.8705314009661835\n",
      "Train Accuracy = 1.0\n",
      "Test Accuracy = 0.9106666666666666\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "train_preds = rf.predict_proba(X_train)\n",
    "test_preds = rf.predict_proba(X_test)\n",
    "train_preds = rf.predict(X_train)\n",
    "test_preds = rf.predict(X_test)\n",
    "print('Train Gini = {}'.format(2 * roc_auc_score(y_train, train_proba[:, 1]) - 1))\n",
    "print('Test Gini = {}'.format(2 * roc_auc_score(y_test, test_proba[:, 1]) - 1))\n",
    "print('Train F1 = {}'.format(f1_score(y_train, train_preds)))\n",
    "print('Test F1 = {}'.format(f1_score(y_test, test_preds)))\n",
    "print('Train Accuracy = {}'.format(accuracy_score(y_train, train_preds)))\n",
    "print('Test Accuracy = {}'.format(accuracy_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini = 0.9444212289143077\n",
      "Test Gini = 0.7496119874844391\n",
      "Train F1 = 0.9990232467278767\n",
      "Test F1 = 0.9125295508274232\n",
      "Train Accuracy = 0.9992857142857143\n",
      "Test Accuracy = 0.9383333333333334\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X_train, y_train)\n",
    "train_preds = lgbm.predict_proba(X_train)\n",
    "test_preds = lgbm.predict_proba(X_test)\n",
    "train_preds = lgbm.predict(X_train)\n",
    "test_preds = lgbm.predict(X_test)\n",
    "print('Train Gini = {}'.format(2 * roc_auc_score(y_train, train_proba[:, 1]) - 1))\n",
    "print('Test Gini = {}'.format(2 * roc_auc_score(y_test, test_proba[:, 1]) - 1))\n",
    "print('Train F1 = {}'.format(f1_score(y_train, train_preds)))\n",
    "print('Test F1 = {}'.format(f1_score(y_test, test_preds)))\n",
    "print('Train Accuracy = {}'.format(accuracy_score(y_train, train_preds)))\n",
    "print('Test Accuracy = {}'.format(accuracy_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini = 0.9985429211742267\n",
      "Test Gini = 0.8636193729678863\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = Seniority_committee(N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making hyperplane number 1\n",
      "X_train.shape[0] = 7000\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -8.871395426494502\n",
      "Time taken for optimization: 12.962341785430908\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.9750887305337979\n",
      "Time taken for optimization: 9.562434673309326\n",
      "X_1.shape[0] = 650\n",
      "X_0.shape[0] = 207\n",
      "\n",
      "Making hyperplane number 2\n",
      "X_train.shape[0] = 6350\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.7394710676374929\n",
      "Time taken for optimization: 15.918854236602783\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -3.922387097379128\n",
      "Time taken for optimization: 11.58742642402649\n",
      "X_1.shape[0] = 114\n",
      "X_0.shape[0] = 400\n",
      "\n",
      "Making hyperplane number 3\n",
      "X_train.shape[0] = 5950\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -2.0866480795278504\n",
      "Time taken for optimization: 13.991222858428955\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.45533829847026147\n",
      "Time taken for optimization: 9.881186962127686\n",
      "X_1.shape[0] = 274\n",
      "X_0.shape[0] = 88\n",
      "\n",
      "Making hyperplane number 4\n",
      "X_train.shape[0] = 5676\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.7500117036197618\n",
      "Time taken for optimization: 12.628959894180298\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -4.383979141476836\n",
      "Time taken for optimization: 10.195430755615234\n",
      "X_1.shape[0] = 74\n",
      "X_0.shape[0] = 640\n",
      "\n",
      "Making hyperplane number 5\n",
      "X_train.shape[0] = 5036\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.42109296775782135\n",
      "Time taken for optimization: 9.572112083435059\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.5475560383823048\n",
      "Time taken for optimization: 8.596084833145142\n",
      "X_1.shape[0] = 75\n",
      "X_0.shape[0] = 94\n",
      "Cutted data shape is not enough\n",
      "\n",
      "k = 6\n",
      "L = 64\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.3754039319099448\n",
      "Time taken for optimization: 11.281328201293945\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -3.591688216154837\n",
      "Time taken for optimization: 10.309393405914307\n",
      "X_1.shape[0] = 34\n",
      "X_0.shape[0] = 559\n",
      "\n",
      "Making hyperplane number 6\n",
      "X_train.shape[0] = 4477\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.5778739006927738\n",
      "Time taken for optimization: 11.027990102767944\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.11481802953745743\n",
      "Time taken for optimization: 8.090137481689453\n",
      "X_1.shape[0] = 73\n",
      "X_0.shape[0] = 30\n",
      "Cutted data shape is not enough\n",
      "\n",
      "k = 6\n",
      "L = 64\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.27577575791947284\n",
      "Time taken for optimization: 12.319937467575073\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.09979696992307505\n",
      "Time taken for optimization: 7.007666826248169\n",
      "X_1.shape[0] = 38\n",
      "X_0.shape[0] = 16\n",
      "Cutted data shape is not enough\n",
      "\n",
      "k = 5\n",
      "L = 32\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.8609485807109047\n",
      "Time taken for optimization: 10.959306955337524\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.8211302104002601\n",
      "Time taken for optimization: 8.178515672683716\n",
      "X_1.shape[0] = 154\n",
      "X_0.shape[0] = 127\n",
      "\n",
      "Making hyperplane number 7\n",
      "X_train.shape[0] = 4323\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.26662046502765413\n",
      "Time taken for optimization: 6.930719614028931\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.1881258424982564\n",
      "Time taken for optimization: 5.587636709213257\n",
      "X_1.shape[0] = 33\n",
      "X_0.shape[0] = 25\n",
      "Cutted data shape is not enough\n",
      "\n",
      "k = 6\n",
      "L = 64\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.3953473352314293\n",
      "Time taken for optimization: 8.548351526260376\n",
      "Optimizing hyperplane for class 0\n",
      "Optimization is started\n",
      "First approximation is obtained\n",
      "The minimum of the loss function: -0.11917758895748932\n",
      "Time taken for optimization: 6.629093885421753\n",
      "X_1.shape[0] = 101\n",
      "X_0.shape[0] = 18\n",
      "\n",
      "Making hyperplane number 8\n",
      "X_train.shape[0] = 4222\n",
      "k = 7\n",
      "L = 128\n",
      "Optimizing hyperplane for class 1\n",
      "Optimization is started\n",
      "First approximation is obtained\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-65034c50bea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TNC'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-75-ee9caff59437>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, optim_method, cycle_range, disp, adaptive, maxiter, xatol, verbose)\u001b[0m\n\u001b[0;32m    328\u001b[0m                                                      \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'maxiter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                                                      \u001b[0mxatol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xatol'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                                                      verbose=verbose)\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-ee9caff59437>\u001b[0m in \u001b[0;36mmake_hyperplane\u001b[1;34m(self, class_num, X_train, optim_method, c, cycle_range, disp, adaptive, maxiter, xatol, verbose)\u001b[0m\n\u001b[0;32m    235\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_train_loss_class_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_train_loss_class_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[1;32m--> 613\u001b[1;33m                              **options)\n\u001b[0m\u001b[0;32m    614\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cobyla'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001b[0m in \u001b[0;36m_minimize_tnc\u001b[1;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    407\u001b[0m                                         \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCGit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                                         \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepmx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                                         xtol, pgtol, rescale, callback)\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0mfunv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjacv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapprox_fprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-ee9caff59437>\u001b[0m in \u001b[0;36mcompute_train_loss_class_1\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-ee9caff59437>\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, X, y, w)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-ee9caff59437>\u001b[0m in \u001b[0;36mprobability\u001b[1;34m(self, X, w)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \"\"\"\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mlinear\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mlinear\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sc.fit(X_train, y_train, optim_method='TNC', maxiter=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (0,\n",
       "  array([ 0.02347743,  0.03994415,  0.03912963,  0.0358237 , -0.07754966,\n",
       "         -0.03582174, -0.02679344, -0.03031107,  0.00074428,  0.02622542,\n",
       "          0.02523019,  0.02103864, -0.04242608]),\n",
       "  0.007710100231303007,\n",
       "  0.4469577415395406),\n",
       " 2: (1,\n",
       "  array([-0.01029176, -0.04451528, -0.02254985, -0.00280012,  0.02892913,\n",
       "          0.00793972,  0.0090729 ,  0.01244328, -0.03594766, -0.01457453,\n",
       "         -0.02816325, -0.03872933,  0.03748206]),\n",
       "  0.9936507936507937,\n",
       "  0.3790656416321703),\n",
       " 3: (1,\n",
       "  array([-0.02795467, -0.00328587, -0.03103596, -0.01494818,  0.04414138,\n",
       "          0.01895   ,  0.00928745, -0.00021212, -0.00978925, -0.02566693,\n",
       "         -0.01719733, -0.02184986,  0.01082632]),\n",
       "  0.989010989010989,\n",
       "  0.3563688407278675),\n",
       " 4: (1,\n",
       "  array([-0.09713729, -0.01018063,  0.0675843 , -0.05768072,  0.04567779,\n",
       "          0.01095149,  0.01698898,  0.00918765,  0.00697072, -0.06239847,\n",
       "         -0.05184121, -0.02097406, -0.00713352]),\n",
       "  0.9739583333333334,\n",
       "  0.3311342838901894),\n",
       " 5: (1,\n",
       "  array([ 0.00388527, -0.04521221,  0.00923407, -0.05677709,  0.01423035,\n",
       "          0.02114968,  0.00151456,  0.01087914, -0.04219584, -0.01102337,\n",
       "         -0.03877673, -0.00086681,  0.01910351]),\n",
       "  0.9915254237288136,\n",
       "  0.314123553809212),\n",
       " 6: (0,\n",
       "  array([ 2.19837114e-02,  2.34156508e-02,  2.63585616e-03,  1.45561005e-02,\n",
       "         -1.41691416e-02,  5.79912154e-03, -3.55096659e-05, -1.80596995e-03,\n",
       "          2.58949949e-02,  3.49182482e-03, -1.85934963e-02, -1.03480417e-02,\n",
       "         -5.33009027e-02]),\n",
       "  0.00847457627118644,\n",
       "  0.3222047949809545),\n",
       " 7: (0,\n",
       "  array([ 0.13674538,  0.01679   ,  0.14763955,  0.1336175 , -0.10891444,\n",
       "         -0.02794992, -0.05212718, -0.0329207 ,  0.08696462, -0.04614319,\n",
       "          0.1100899 ,  0.08512403, -0.29522099]),\n",
       "  0.05891472868217054,\n",
       "  0.3666841278156103),\n",
       " 8: (1,\n",
       "  array([-0.03158856,  0.03232546, -0.03386395, -0.09117199,  0.02726443,\n",
       "         -0.00082142,  0.02116245,  0.02134991, -0.02039501, -0.02994589,\n",
       "         -0.01859678,  0.01488696, -0.00514162]),\n",
       "  0.9673202614379085,\n",
       "  0.34160982264665757),\n",
       " 9: (1,\n",
       "  array([  1.09053268,  -0.54095295, -12.21641179,  -1.87862443,\n",
       "           2.3678001 ,   1.2152325 ,   1.00570486,  -0.30579536,\n",
       "          -2.89347444,  -3.60735391,   2.67092349,  -3.89374558,\n",
       "           1.68152043]),\n",
       "  0.9776119402985075,\n",
       "  0.3174738034551119),\n",
       " 10: (0,\n",
       "  array([ 0.02982808,  0.02209916,  0.02032051, -0.00375717, -0.00130837,\n",
       "         -0.00674331, -0.00124003, -0.00591101,  0.03958327, -0.00407513,\n",
       "          0.02203381, -0.02735247, -0.07091554]),\n",
       "  0.03496503496503497,\n",
       "  0.3293978748524203),\n",
       " 11: (1,\n",
       "  array([-5.25304382, -7.9105291 ,  0.56870088,  2.19163741,  1.80267495,\n",
       "          1.33238911, -0.10128916,  1.12017347,  0.17397713,  0.38647806,\n",
       "         -2.45270596, -1.91450772,  0.04448714]),\n",
       "  0.9383561643835616,\n",
       "  0.30197409006785936),\n",
       " 12: (0,\n",
       "  array([  6.83550258,  10.62985278,   6.21002217,  16.62994425,\n",
       "         -28.48161133, -16.36007066, -19.19076006,   9.67661784,\n",
       "           6.1705626 ,  17.83170496, -10.3283222 ,  12.10021579,\n",
       "         -22.55967861]),\n",
       "  0.053691275167785234,\n",
       "  0.31393469123828),\n",
       " 13: (1,\n",
       "  array([-9.01588424,  1.25808695, -6.63600918,  1.24012384,  2.52547314,\n",
       "         -0.3018276 ,  0.33082013,  1.13448104,  1.06192309, -2.86840238,\n",
       "          0.26747239, -3.6156502 ,  1.36490444]),\n",
       "  0.9318181818181818,\n",
       "  0.28638973319824385),\n",
       " 14: (0,\n",
       "  array([ 2.22512031,  1.45152643, -1.02507642,  3.10644614, -6.86624169,\n",
       "         -1.54126441,  0.64068821, -2.4077431 ,  0.22269783,  3.96026209,\n",
       "         -7.92716959,  3.46366032, -2.35916443]),\n",
       "  0.08379888268156424,\n",
       "  0.2994248741912293),\n",
       " 15: (0,\n",
       "  array([ 1.71947139,  5.66608363,  3.88045527,  6.12166821, -5.85085464,\n",
       "         -3.63379186, -1.54225141, -2.46008457,  3.04589278,  4.04070444,\n",
       "          3.54380395,  4.47461158, -9.24159973]),\n",
       "  0.14605809128630706,\n",
       "  0.4166138237159163),\n",
       " 16: (0,\n",
       "  array([ 0.32967677,  4.70219621,  9.97867611,  3.44466885, -9.94914501,\n",
       "         -1.80324618, -2.79366801, -6.85995971,  5.06273023,  7.82556476,\n",
       "         11.55247022, -2.38267485, -8.83252947]),\n",
       "  0.25524475524475526,\n",
       "  0.5084577114427861),\n",
       " 17: (1,\n",
       "  array([  0.70148363,   1.79151345,   0.98019584,  -7.59185778,\n",
       "           4.14461039,   0.5426388 ,   3.32855882,  -1.04481086,\n",
       "         -10.76102288,  -6.85550395,  -5.43932292,  -1.00036533,\n",
       "           4.16663905]),\n",
       "  0.896551724137931,\n",
       "  0.4578177727784027),\n",
       " 18: (0,\n",
       "  array([ 0.34028365,  0.05883136,  1.10483854,  6.75878289, -4.29343595,\n",
       "          0.28360562, -0.30246753, -2.64879774,  5.04876156, -0.63168033,\n",
       "         -2.09325893,  7.43536102, -7.24088022]),\n",
       "  0.24154589371980675,\n",
       "  0.5234604105571847),\n",
       " 19: (1,\n",
       "  array([ 2.34662633, -9.8077517 , -5.11337651, -7.60008966,  3.16599766,\n",
       "          1.15919232,  1.95395009, -1.39195776, -1.25995604,  1.2457703 ,\n",
       "          1.29339173, -2.16789489,  2.70329282]),\n",
       "  0.8248175182481752,\n",
       "  0.44770642201834865),\n",
       " 20: (0,\n",
       "  array([ 3.99017418,  1.39467108, -3.3090459 , -5.13335748,  0.83096449,\n",
       "         -8.12803663,  3.56870592, -3.99429641,  5.4147021 ,  7.32252423,\n",
       "         -0.72572681, -1.46180801, -3.00377206]),\n",
       "  0.20168067226890757,\n",
       "  0.5164319248826291),\n",
       " 21: (0,\n",
       "  array([ 3.46075226e+00,  1.17918007e+00,  1.65060703e+01,  3.57023050e+00,\n",
       "         -1.14175764e+01, -1.91299161e+00,  8.00462521e-03, -6.51835574e+00,\n",
       "          7.49627807e+00,  7.71340870e+00,  9.11737266e+00,  9.60755897e+00,\n",
       "         -1.18110824e+01]),\n",
       "  0.3824561403508772,\n",
       "  0.7872340425531915)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.weights_hp # optim_weights = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(optim_weights).to_pickle('weights_without_last_list.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_1 = Seniority_committee(N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_1.weights_hp = sc.weights_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict the targets: 5.277034044265747\n"
     ]
    }
   ],
   "source": [
    "train_proba = sc_1.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict the targets: 2.3339550495147705\n"
     ]
    }
   ],
   "source": [
    "test_proba = sc_1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini = 0.8849357372714146\n"
     ]
    }
   ],
   "source": [
    "print('Train Gini = {}'.format(2 * roc_auc_score(y_train, train_proba) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gini = 0.8473687817743731\n"
     ]
    }
   ],
   "source": [
    "print('Test Gini = {}'.format(2 * roc_auc_score(y_test, test_proba) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict the targets: 6.230452537536621\n",
      "Time taken to predict the targets: 2.4784576892852783\n",
      "Train F1 = 0.8521551724137931\n",
      "Test F1 = 0.8302277432712215\n"
     ]
    }
   ],
   "source": [
    "train_preds = sc_1.predict(X_train)\n",
    "test_preds = sc_1.predict(X_test)\n",
    "print('Train F1 = {}'.format(f1_score(y_train, train_preds)))\n",
    "print('Test F1 = {}'.format(f1_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.902\n",
      "Test accuracy = 0.8906666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy = {}'.format(accuracy_score(y_train, train_preds)))\n",
    "print('Test accuracy = {}'.format(accuracy_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seniority_committee:\n",
    "    \"\"\"\n",
    "        ML Algo based on the seniority committee method\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \"\"\"\n",
    "            :param N: число наблюдений, находящихся выше гиперплоскости\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.L = -1\n",
    "        self.N = N\n",
    "        self.weights_hp = dict() # здесь будут записаны оптимальные веса гиперплоскостей в виде: \n",
    "                                 # {'num_hyperplane': (voted_class, optim_weights, probability)}\n",
    "        self.optim_params = dict() # здесь будут записаны параметры оптимизации гиперплоскостей в виде:\n",
    "                   # {'cycle_range': int, 'disp': bool, 'adaptive': bool, 'maxiter': int or None, 'xatol': float or None}\n",
    "                   # значение параметров дано в описании метода make_hyperplane\n",
    "        \n",
    "    def expand(self, X):\n",
    "        \"\"\"\n",
    "            Concatenates the feature matrix with the column of ones\n",
    "            \n",
    "            :param X: матрица признаков\n",
    "            :returns: исходная матрица признаков со столбцом единиц (добавили bias)\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        ones_col = np.ones(len(X_new))\n",
    "        ones_col.shape = (len(X_new), 1)\n",
    "        X_new = np.hstack((X_new, ones_col))\n",
    "        return X_new\n",
    "    \n",
    "    \n",
    "    def probability(self, X, w):\n",
    "        \"\"\"\n",
    "            Принимает на вход матрицу фичей и вектор весов\n",
    "            Возвращает предсказание вероятность того, что y = 1 при фиксированных x, P(y=1|x)\n",
    "\n",
    "            :param X: матрица признаков \n",
    "            :param w: вектор весов\n",
    "            :returns: вероятность того, что y = 1 при фиксированных x, P(y=1|x) ## вектор вероятностей = ReLU1(X.T*w)\n",
    "        \"\"\"\n",
    "        \n",
    "        linear = np.dot(X, w)\n",
    "        linear[linear < 0] = 0\n",
    "        linear[linear > 1] = 1\n",
    "        \n",
    "        return linear\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, X, y, w):\n",
    "        \"\"\"\n",
    "            Принимает на вход матрицу весов, вектор ответов, вектор весов и параметр L, \n",
    "            влияющий на долю класса 0 в отсекающей гиперплоскости.\n",
    "            Выдаёт на выход значение функции потерь\n",
    "            \n",
    "            :param X: матрица признаков\n",
    "            :param w: вектор целевой переменной\n",
    "            :param w: вектор весов\n",
    "            :returns: значение функции потерь\n",
    "        \"\"\"\n",
    "        \n",
    "        p1 = self.probability(X, w)\n",
    "        loss = np.sum((self.L - (self.L + 1) * y) * p1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def compute_train_loss_class_0(self, w):\n",
    "        \"\"\"\n",
    "            Function that we want to minimize, the committee member votes for class 0\n",
    "            \n",
    "            :param w: вектор весов\n",
    "            :returns: значение функции потерь на обучающей выборке, когда член коммитета голосует за класс 0\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.X_train is None or self.y_train is None:\n",
    "            raise Exception('Model is not fitted')\n",
    "        \n",
    "        return self.compute_loss(self.X_train, 1 - self.y_train, w)\n",
    "    \n",
    "    \n",
    "    def compute_train_loss_class_1(self, w):\n",
    "        \"\"\"\n",
    "            Function that we want to minimize, the committee member votes for class 1\n",
    "            \n",
    "            :param w: вектор весов\n",
    "            :returns: значение функции потерь на обучающей выборке, когда член коммитета голосует за класс 1\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.X_train is None or self.y_train is None:\n",
    "            raise Exception('Model is not fitted')\n",
    "        \n",
    "        return self.compute_loss(self.X_train, self.y_train, w)\n",
    "    \n",
    "    \n",
    "    def make_hyperplane(self, class_num, X_train, optim_method, c=0.1, cycle_range=100, disp=False, \\\n",
    "                        adaptive=True, maxiter=None, xatol=None, verbose=0):\n",
    "        \"\"\"\n",
    "            Function that makes one of three hyperplanes\n",
    "            \n",
    "            :param class_num: класс, за который голосует данный член комитета: [0, 1]\n",
    "            :param X_train: матрица признаков обучающей выборки\n",
    "            :param optim_method: метод оптимизации: ['Nelder-Mead', 'differential_evolution', 'BFGS', 'CG', 'SLSQP', \\\n",
    "                                                    'COBYLA', 'TNC']\n",
    "            :param с: вспомогательный коэффициент для выбора начального приближения\n",
    "            :param cycle_range: количество итераций минимизации функции потерь\n",
    "            Параметры оптимизации с помощью алгоритма Нелдера-Мида:\n",
    "                :param disp: bool: печать сообщения о сходимости\n",
    "                :param adaptive: bool: адаптация параметров алгоритма для размерности задачи (полезно при больших размерностях)\n",
    "                :param maxiter: максимально допустимое количество итераций при оптимизации\n",
    "                :param xatol: абсолютная ошибка на оптимальных точках между итерациями, приемлемая для сходимости\n",
    "            :param verbose: подробный вывод описания обучения: [0, 1, 2]:\n",
    "                0 - не печатать ничего\n",
    "                1 - печатать общее время обучения\n",
    "                2 - подробный вывод информации о процессе обучения\n",
    "            :returns: значение функции потерь на тестовой выборке\n",
    "        \"\"\"\n",
    "\n",
    "        if self.X_train is None or self.y_train is None:\n",
    "            raise Exception('Using make_hyperplane method before fitting')\n",
    "        if class_num not in [0, 1]:\n",
    "            raise Exception('Only binary classification is available, class_num should be 0 or 1')\n",
    "        if optim_method not in ['Nelder-Mead', 'differential_evolution', 'BFGS', 'CG', 'SLSQP', 'COBYLA', 'TNC']:\n",
    "            raise Exception(\"Unavailable optimization method, only ['Nelder-Mead', 'differential_evolution', 'BFGS', 'CG', 'SLSQP', 'COBYLA', 'TNC'] are available\")\n",
    "        \n",
    "        optim_result = []\n",
    "        optim_result_more_precise = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if verbose == 2:\n",
    "            print('Optimization is started')\n",
    "        \n",
    "        if optim_method == 'Nelder-Mead':\n",
    "            \n",
    "            for i in range(cycle_range):\n",
    "\n",
    "                start_w = np.array((np.random.rand(X_train.shape[1]) - 0.5) * c)\n",
    "\n",
    "                if class_num == 1:\n",
    "\n",
    "                        res = minimize(self.compute_train_loss_class_1, x0=start_w, method='Nelder-Mead', \\\n",
    "                                       options={'disp': disp,'adaptive': adaptive, 'maxiter': maxiter, 'xatol': xatol})\n",
    "\n",
    "                elif class_num == 0:\n",
    "\n",
    "                        res = minimize(self.compute_train_loss_class_0, x0=start_w, method='Nelder-Mead', \\\n",
    "                                       options={'disp': disp,'adaptive': adaptive, 'maxiter': maxiter, 'xatol': xatol})\n",
    "\n",
    "                optim_result.append([res.fun, res.x])\n",
    "\n",
    "\n",
    "            if optim_result == []:\n",
    "                raise Exception('We\\'ve not get any satisfying first approximation')\n",
    "\n",
    "            optim_result.sort(key=lambda x: x[0])\n",
    "            optim_result = [x[1] for x in optim_result][:int(0.1 * cycle_range)]\n",
    "            \n",
    "            if verbose == 2:\n",
    "                print('First approximation is obtained')\n",
    "\n",
    "            for start_w_new in optim_result:\n",
    "\n",
    "                if class_num == 1:\n",
    "                    res = minimize(self.compute_train_loss_class_1, x0=start_w_new, method='Nelder-Mead', \\\n",
    "                             options={'disp': False, 'adaptive':True, 'xatol':1})\n",
    "                elif class_num == 0:\n",
    "                    res = minimize(self.compute_train_loss_class_0, x0=start_w_new, method='Nelder-Mead', \\\n",
    "                             options={'disp': False, 'adaptive':True, 'xatol':1})\n",
    "\n",
    "                for k in range(2):\n",
    "                    if class_num == 1:\n",
    "                        res = minimize(self.compute_train_loss_class_1, x0=res.x, method='Nelder-Mead', \\\n",
    "                                 options={'disp': False, 'adaptive':True})\n",
    "                    elif class_num == 0:\n",
    "                        res = minimize(self.compute_train_loss_class_0, x0=res.x, method='Nelder-Mead', \\\n",
    "                                 options={'disp': False, 'adaptive':True})\n",
    "\n",
    "                optim_result_more_precise += [[res.fun, res.x]]\n",
    "                \n",
    "            optim_result_more_precise = pd.DataFrame(optim_result_more_precise)\n",
    "            optim_result_more_precise = optim_result_more_precise.sort_values(0).head(1)\n",
    "            hyperplane_coefficients = optim_result_more_precise[1].values[0]\n",
    "            min_loss_func = optim_result_more_precise[0].values[0]\n",
    "            \n",
    "        elif optim_method == 'TNC':\n",
    "            \n",
    "            for i in range(cycle_range):\n",
    "\n",
    "                start_w = np.array((np.random.rand(X_train.shape[1]) - 0.5) * c)\n",
    "\n",
    "                if class_num == 1:\n",
    "\n",
    "                        res = minimize(self.compute_train_loss_class_1, x0=start_w, method=optim_method, jac='3-point', \\\n",
    "                                       options={'disp': False, 'maxCGit': 0, 'eta': 1, 'stepmx': 5, 'rescale': 0})\n",
    "\n",
    "                elif class_num == 0:\n",
    "\n",
    "                        res = minimize(self.compute_train_loss_class_0, x0=start_w, method=optim_method, jac='3-point', \\\n",
    "                                       options={'disp': False, 'maxCGit': 0, 'eta': 1, 'stepmx': 5, 'rescale': 0})\n",
    "\n",
    "                optim_result.append([res.fun, res.x])\n",
    "\n",
    "\n",
    "            if optim_result == []:\n",
    "                raise Exception('We\\'ve not get any satisfying first approximation')\n",
    "\n",
    "            optim_result.sort(key=lambda x: x[0])\n",
    "            optim_result = [x[1] for x in optim_result][:int(0.1 * cycle_range)]\n",
    "            \n",
    "            if verbose == 2:\n",
    "                print('First approximation is obtained')\n",
    "\n",
    "            for start_w_new in optim_result:\n",
    "\n",
    "                if class_num == 1:\n",
    "                    res = minimize(self.compute_train_loss_class_1, x0=start_w_new, method=optim_method, jac='3-point', \\\n",
    "                                   options={'disp': disp, 'maxCGit': 0, 'eta': 1, 'stepmx': 5, 'rescale': 0})\n",
    "                elif class_num == 0:\n",
    "                    res = minimize(self.compute_train_loss_class_0, x0=start_w_new, method=optim_method, jac='3-point', \\\n",
    "                                   options={'disp': disp, 'maxCGit': 0, 'eta': 1, 'stepmx': 5, 'rescale': 0})\n",
    "\n",
    "                for k in range(2):\n",
    "                    if class_num == 1:\n",
    "                        res = minimize(self.compute_train_loss_class_1, x0=res.x, method=optim_method, jac='3-point', \\\n",
    "                                       options={'disp': disp, 'maxCGit': 0, 'eta': 1, 'stepmx': 5, 'rescale': 0})\n",
    "                    elif class_num == 0:\n",
    "                        res = minimize(self.compute_train_loss_class_0, x0=res.x, method=optim_method, jac='3-point', \\\n",
    "                                       options={'disp': disp, 'maxCGit': 0, 'eta': 1, 'stepmx': 5, 'rescale': 0})\n",
    "\n",
    "                optim_result_more_precise += [[res.fun, res.x]]\n",
    "                \n",
    "            optim_result_more_precise = pd.DataFrame(optim_result_more_precise)\n",
    "            optim_result_more_precise = optim_result_more_precise.sort_values(0).head(1)\n",
    "            hyperplane_coefficients = optim_result_more_precise[1].values[0]\n",
    "            min_loss_func = optim_result_more_precise[0].values[0]\n",
    "            \n",
    "        elif optim_method == 'differential_evolution':\n",
    "            \n",
    "            bounds = []\n",
    "            for j in range(X_train.shape[1]):\n",
    "                bounds.append((-10, 10))\n",
    "                \n",
    "            if class_num == 1:\n",
    "                res = differential_evolution(self.compute_train_loss_class_1, bounds=bounds)\n",
    "            \n",
    "            if class_num == 0:\n",
    "                res = differential_evolution(self.compute_train_loss_class_0, bounds=bounds)\n",
    "            \n",
    "            hyperplane_coefficients = res.x\n",
    "            min_loss_func = res.fun\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            for i in range(cycle_range):\n",
    "\n",
    "                start_w = np.array((np.random.rand(X_train.shape[1]) - 0.5) * c)\n",
    "\n",
    "                if class_num == 1:\n",
    "\n",
    "                        res = minimize(self.compute_train_loss_class_1, x0=start_w, method=optim_method)\n",
    "\n",
    "                elif class_num == 0:\n",
    "\n",
    "                        res = minimize(self.compute_train_loss_class_0, x0=start_w, method=optim_method)\n",
    "\n",
    "                optim_result.append([res.fun, res.x])\n",
    "\n",
    "\n",
    "            if optim_result == []:\n",
    "                raise Exception('We\\'ve not get any satisfying first approximation')\n",
    "\n",
    "            optim_result.sort(key=lambda x: x[0])\n",
    "            optim_result = [x[1] for x in optim_result][:int(0.1 * cycle_range)]\n",
    "            \n",
    "            if verbose == 2:\n",
    "                print('First approximation is obtained')\n",
    "\n",
    "            for start_w_new in optim_result:\n",
    "\n",
    "                if class_num == 1:\n",
    "                    res = minimize(self.compute_train_loss_class_1, x0=start_w_new, method=optim_method)\n",
    "                elif class_num == 0:\n",
    "                    res = minimize(self.compute_train_loss_class_0, x0=start_w_new, method=optim_method)\n",
    "\n",
    "                for k in range(2):\n",
    "                    if class_num == 1:\n",
    "                        res = minimize(self.compute_train_loss_class_1, x0=res.x, method=optim_method)\n",
    "                    elif class_num == 0:\n",
    "                        res = minimize(self.compute_train_loss_class_0, x0=res.x, method=optim_method)\n",
    "\n",
    "                optim_result_more_precise += [[res.fun, res.x]]\n",
    "                \n",
    "            optim_result_more_precise = pd.DataFrame(optim_result_more_precise)\n",
    "            optim_result_more_precise = optim_result_more_precise.sort_values(0).head(1)\n",
    "            hyperplane_coefficients = optim_result_more_precise[1].values[0]\n",
    "            min_loss_func = optim_result_more_precise[0].values[0]\n",
    "        \n",
    "        if verbose == 2:\n",
    "            print('The minimum of the loss function: {0}'.format(min_loss_func))\n",
    "            print('Time taken for optimization: {0}'.format(time.time() - start_time))\n",
    "\n",
    "        return hyperplane_coefficients\n",
    "        \n",
    "    \n",
    "    def cutter(self, X, w):\n",
    "        \"\"\"\n",
    "            Function that makes binary targets for rational numbers\n",
    "            \n",
    "            :param X: матрица признаков\n",
    "            :param w: вектор оптимальных весов\n",
    "            :returns: бинаризованные предсказания целевой переменной\n",
    "        \"\"\"\n",
    "        \n",
    "        linear = np.dot(X, w)\n",
    "        linear[linear < 0] = 0\n",
    "        \n",
    "        return np.sign(linear)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, optim_method='Nelder-Mead', cycle_range=100, disp=False, adaptive=True, maxiter=10, xatol=0.3, \\\n",
    "           verbose=0):\n",
    "        \"\"\"\n",
    "            Fits the algorithm on the train sample \n",
    "            \n",
    "            :param X: матрица признаков, обучающая выборка\n",
    "            :param y: вектор истинных значений целевой переменной обучающей выборки\n",
    "            :param optim_method: метод оптимизации: ['Nelder-Mead', 'differential_evolution', 'BFGS', 'CG', 'SLSQP', \\\n",
    "                                                    'COBYLA', 'TNC']\n",
    "            Параметры оптимизации с помощью алгоритма Нелдера-Мида:\n",
    "                :param disp: bool: печать сообщения о сходимости\n",
    "                :param adaptive: bool: адаптация параметров алгоритма для размерности задачи (полезно при больших размерностях)\n",
    "                :param maxiter: максимально допустимое количество итераций при оптимизации\n",
    "                :param xatol: абсолютная ошибка на оптимальных точках между итерациями, приемлемая для сходимости\n",
    "            :param verbose: подробный вывод описания обучения: [0, 1, 2]:\n",
    "                0 - не печатать ничего\n",
    "                1 - печатать общее время обучения\n",
    "                2 - подробный вывод информации о процессе обучения\n",
    "            :returns: -, но на выходе обученная моделька\n",
    "        \"\"\"\n",
    "        \n",
    "        start_of_fit_time = time.time()\n",
    "        \n",
    "        X_new = self.expand(X)\n",
    "        \n",
    "        self.X_train = X_new\n",
    "        self.y_train = y\n",
    "        self.optim_params = {'cycle_range': cycle_range, 'disp': disp, 'adaptive': adaptive, \\\n",
    "                             'maxiter': maxiter, 'xatol': xatol}\n",
    "        \n",
    "        hp_num = 1\n",
    "        \n",
    "        while self.X_train.shape[0] > 2 * self.N:\n",
    "            \n",
    "            if verbose == 2:\n",
    "                print('Making hyperplane number {}'.format(hp_num))\n",
    "                print('X_train.shape[0] = {}'.format(self.X_train.shape[0]))\n",
    "            \n",
    "            k = 10\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                if verbose == 2:\n",
    "                    print('k = {}'.format(k))\n",
    "                \n",
    "                self.L = 2 ** k\n",
    "                \n",
    "                if verbose == 2:\n",
    "                    print('L = {}'.format(self.L))\n",
    "                \n",
    "                if verbose == 2:\n",
    "                    print('Optimizing hyperplane for class 1')\n",
    "                \n",
    "                hp_weights_class_1 = self.make_hyperplane(class_num=1, X_train=self.X_train, \\\n",
    "                                                     optim_method=optim_method, \\\n",
    "                                                     c=0.1, cycle_range=self.optim_params['cycle_range'], \\\n",
    "                                                     disp=self.optim_params['disp'], \\\n",
    "                                                     adaptive=self.optim_params['adaptive'], \\\n",
    "                                                     maxiter=self.optim_params['maxiter'], \\\n",
    "                                                     xatol=self.optim_params['xatol'], \\\n",
    "                                                     verbose=verbose)\n",
    "                \n",
    "                if verbose == 2:\n",
    "                    print('Optimizing hyperplane for class 0')\n",
    "                \n",
    "                hp_weights_class_0 = self.make_hyperplane(class_num=0, X_train=self.X_train, \\\n",
    "                                                     optim_method=optim_method, \\\n",
    "                                                     c=0.1, cycle_range=self.optim_params['cycle_range'], \\\n",
    "                                                     disp=self.optim_params['disp'], \\\n",
    "                                                     adaptive=self.optim_params['adaptive'], \\\n",
    "                                                     maxiter=self.optim_params['maxiter'], \\\n",
    "                                                     xatol=self.optim_params['xatol'], \\\n",
    "                                                     verbose=verbose)\n",
    "                \n",
    "                cut_1 = self.cutter(self.X_train, hp_weights_class_1)\n",
    "                cut_0 = self.cutter(self.X_train, hp_weights_class_0)\n",
    "                X_1 = self.X_train[cut_1 == 1]\n",
    "                y_1 = self.y_train[cut_1 == 1]\n",
    "                y_1_rest = self.y_train[cut_1 == 0] # оставшиеся после отсечения сэмплы\n",
    "                X_0 = self.X_train[cut_0 == 1]\n",
    "                y_0 = self.y_train[cut_0 == 1]\n",
    "                y_0_rest = self.y_train[cut_0 == 0] # оставшиеся после отсечения сэмплы\n",
    "                \n",
    "                if verbose == 2:\n",
    "                    print('X_1.shape[0] = {}'.format(X_1.shape[0]))\n",
    "                    print('X_0.shape[0] = {}'.format(X_0.shape[0]))\n",
    "                \n",
    "                if X_1.shape[0] < self.N and X_0.shape[0] < self.N:\n",
    "                    \n",
    "                    if verbose == 2:\n",
    "                        print('Cutted data shape is not enough\\n')\n",
    "                    k -= 1\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if X_1.shape[0] >= X_0.shape[0]:\n",
    "                        \n",
    "                        proba = y_1.sum() / len(y_1)\n",
    "                        proba_rest = y_1_rest.sum() / len(y_1_rest)\n",
    "                        self.weights_hp[hp_num] = (1, hp_weights_class_1, proba, proba_rest)\n",
    "                        \n",
    "                        self.X_train = self.X_train[cut_1 == 0]\n",
    "                        self.y_train = self.y_train[cut_1 == 0]\n",
    "                        \n",
    "                    elif X_1.shape[0] < X_0.shape[0]:\n",
    "                        \n",
    "                        proba = y_0.sum() / len(y_0)\n",
    "                        proba_rest = y_0_rest.sum() / len(y_0_rest)\n",
    "                        self.weights_hp[hp_num] = (0, hp_weights_class_0, proba, proba_rest)\n",
    "                        \n",
    "                        self.X_train = self.X_train[cut_0 == 0]\n",
    "                        self.y_train = self.y_train[cut_0 == 0]\n",
    "                        \n",
    "                    hp_num += 1\n",
    "                    if verbose == 2:\n",
    "                        print()\n",
    "                    break\n",
    "                    \n",
    "        end_of_fit_time = time.time()\n",
    "        \n",
    "        if verbose == 2 or verbose == 1:\n",
    "            print('Time taken to fit the model: {0}'.format(end_of_fit_time - start_of_fit_time))\n",
    "                \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "            Makes predict_proba for the sample\n",
    "            \n",
    "            :param X: матрица признаков, обучающая выборка\n",
    "            :returns: предсказания вероятностей отнесения к классу 1\n",
    "        \"\"\"\n",
    "        \n",
    "        start_of_predict_time = time.time()\n",
    "        \n",
    "        X_new = self.expand(X)\n",
    "        \n",
    "        test_ = X_new.copy()\n",
    "        \n",
    "        if self.weights_hp == {}:\n",
    "            raise Exception('Model is not fitted')\n",
    "        \n",
    "        predictions = pd.DataFrame({'proba': np.zeros(X.shape[0]), 'is_scored': False})\n",
    "        \n",
    "        for hp_num in self.weights_hp.keys():\n",
    "            \n",
    "            class_num, weights, proba, proba_rest = self.weights_hp[hp_num]\n",
    "            \n",
    "            cut = self.cutter(test_, weights)\n",
    "            \n",
    "            X_ = test_[cut == 1]\n",
    "            test_ = test_[cut == 0]\n",
    "            \n",
    "            k = 0\n",
    "            for i in range(predictions.shape[0]):\n",
    "                \n",
    "                if predictions.loc[i, 'is_scored'] == False:\n",
    "                    if cut[k] == True:\n",
    "                        predictions.loc[i, 'proba'] = proba\n",
    "                        predictions.loc[i, 'is_scored'] = True\n",
    "                    k += 1\n",
    "            if hp_num == list(self.weights_hp.keys())[-1]:\n",
    "                for i in range(predictions.shape[0]):\n",
    "                    if predictions.loc[i, 'is_scored'] == False:\n",
    "                        predictions.loc[i, 'proba'] = proba_rest\n",
    "                        predictions.loc[i, 'is_scored'] = True\n",
    "                        \n",
    "        end_of_predict_time = time.time()\n",
    "        print('Time taken to predict the targets: {0}'.format(end_of_predict_time - start_of_predict_time))\n",
    "        \n",
    "        return predictions['proba'].values\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Makes predict_proba for the sample\n",
    "            \n",
    "            :param X: матрица признаков, обучающая выборка\n",
    "            :returns: предсказания классов\n",
    "        \"\"\"\n",
    "        \n",
    "        proba = self.predict_proba(X)\n",
    "        return np.array([round(x) for x in proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
